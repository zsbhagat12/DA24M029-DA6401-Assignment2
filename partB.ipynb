{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install torchsummary\n",
    "!pip install wandb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51001a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224  # Resize to ImageNet standard\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(\"nature_12K\", \"inaturalist_12K\")\n",
    "full_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n",
    "best_model_path = \"best_models\"\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38879f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your train_dir and val_dir properly\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")  # replace with actual path\n",
    "# val_dir = os.path.join(data_dir, \"train\")  # optional if splitting from train_dir\n",
    "\n",
    "\n",
    "# === get_data_loaders === #\n",
    "def get_data_loaders(config, train_dir=train_dir):\n",
    "    BATCH_SIZE = config.get(\"batch_size\", 64)\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    if config.get(\"augment\", False):\n",
    "        print(\"Applying full data augmentation\")\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.02),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "    else:\n",
    "        print(\"Minimal preprocessing, no augmentation\")\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    # Load dataset\n",
    "    full_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "    targets = np.array(full_dataset.targets)\n",
    "\n",
    "    # Stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(datasets.ImageFolder(root=train_dir, transform=transform_val), val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"Total images: {len(full_dataset)}\")\n",
    "    print(f\"Training set: {len(train_dataset)} images\")\n",
    "    print(f\"Validation set: {len(val_dataset)} images\")\n",
    "    print(f\"Number of classes: {len(full_dataset.classes)}\")\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e420004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in tqdm(loader, desc=f\"Train Epoch {epoch}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    wandb.log({\"train/loss\": epoch_loss, \"train/acc\": epoch_acc, \"epoch\": epoch})\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Eval Epoch {epoch}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    epoch_loss = val_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    wandb.log({\"val/loss\": epoch_loss, \"val/acc\": epoch_acc, \"epoch\": epoch})\n",
    "    print(f\"Validation Loss: {epoch_loss:.4f}, Validation Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8f959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_NAME = \"da24m029-da6401-assignment2\"  # Replace with your project name\n",
    "ENTITY_NAME = \"da24m029-indian-institute-of-technology-madras\"  # Replace with your entity name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38d71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Configurable ===\n",
    "BACKBONE = \"resnet50\"  # Options: 'resnet50', 'vgg16', 'efficientnet_v2_s', 'inception_v3', 'vit_b_16'\n",
    "FREEZE_STRATEGY = \"partial_percent\"  # Options: 'last_only', 'partial_k', 'partial_percent'\n",
    "K = 10  # Used if FREEZE_STRATEGY == 'partial_k'\n",
    "PERCENT = 0.7  # Used if FREEZE_STRATEGY == 'partial_percent'\n",
    "\n",
    "def get_pretrained_model(backbone_name, num_classes):\n",
    "    if backbone_name == \"resnet50\":\n",
    "        model = models.resnet50(weights='DEFAULT')\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        feature_layers = list(model.children())[:-1]\n",
    "    elif backbone_name == \"vgg16\":\n",
    "        model = models.vgg16(weights='DEFAULT')\n",
    "        in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "        feature_layers = list(model.features)\n",
    "    elif backbone_name == \"efficientnet_v2_s\":\n",
    "        model = models.efficientnet_v2_s(weights='DEFAULT')\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        feature_layers = list(model.features)\n",
    "    elif backbone_name == \"inception_v3\":\n",
    "        model = models.inception_v3(weights='DEFAULT', aux_logits=False)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        feature_layers = list(model.children())[:-1]\n",
    "    elif backbone_name == \"vit_b_16\":\n",
    "        model = models.vit_b_16(weights='DEFAULT')\n",
    "        in_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(in_features, num_classes)\n",
    "        feature_layers = list(model.children())[:-1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    return model#, feature_layers\n",
    "\n",
    "# def apply_freezing_strategy(model, feature_layers, strategy, k=None, percent=None):\n",
    "def apply_freezing_strategy(model, strategy, k=None, percent=None):\n",
    "    all_params = list(model.parameters())\n",
    "\n",
    "    if strategy == \"last_only\":\n",
    "        for param in all_params:\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze only classifier\n",
    "        for param in model.parameters():\n",
    "            if param.ndim > 1 and param.requires_grad == False:\n",
    "                continue\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif strategy == \"partial_k\":\n",
    "        for idx, param in enumerate(all_params):\n",
    "            param.requires_grad = idx >= k\n",
    "\n",
    "    elif strategy == \"partial_percent\":\n",
    "        freeze_until = int(len(all_params) * percent)\n",
    "        for idx, param in enumerate(all_params):\n",
    "            param.requires_grad = idx >= freeze_until\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown freezing strategy\")\n",
    "\n",
    "def finetune_model(backbone=BACKBONE, strategy=FREEZE_STRATEGY, k=K, percent=PERCENT, num_classes=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model, feature_layers = get_pretrained_model(backbone, num_classes)\n",
    "    model = get_pretrained_model(backbone, num_classes)\n",
    "\n",
    "    # Apply freezing\n",
    "    # apply_freezing_strategy(model, feature_layers, strategy, k, percent)\n",
    "    apply_freezing_strategy(model, strategy, k, percent)\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"Model: {backbone}, Strategy: {strategy} → Ready for training.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finetune_training(\n",
    "    backbone=\"resnet50\", strategy=\"last_only\", num_epochs=10, batch_size=32,\n",
    "    k=10, percent=0.7, num_classes=10\n",
    "):\n",
    "    # wandb init\n",
    "    wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=ENTITY_NAME,\n",
    "        config={\n",
    "            \"backbone\": backbone,\n",
    "            \"strategy\": strategy,\n",
    "            \"epochs\": num_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"freeze_k\": k,\n",
    "            \"freeze_percent\": percent,\n",
    "            # \"augment\": False\n",
    "        }\n",
    "    )\n",
    "    wandb.run.name = f\"backbone={backbone}, strategy={strategy}, epochs={num_epochs}, batch_size={batch_size}, freeze_k={k}, freeze_percent={percent}\"\n",
    "    wandb.run.save()\n",
    "    \n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader, val_loader = get_data_loaders(config)\n",
    "\n",
    "    # Load model\n",
    "    model = finetune_model(backbone=config.backbone,\n",
    "                           strategy=config.strategy,\n",
    "                           k=config.freeze_k,\n",
    "                           percent=config.freeze_percent,\n",
    "                           num_classes=num_classes)\n",
    "\n",
    "    # Optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        evaluate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "    torch.save(model, os.path.join(best_model_path, \"partB.pth\"))\n",
    "    wandb.save(\"best_model_B.pth\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f6993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Jamie\\IIT Madras\\Semester II_1st Year_2024-25\\DA6401 Introduction to Deep Learning\\Assignments\\2\\Github\\wandb\\run-20250418_214244-7ofgzkbv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2/runs/7ofgzkbv' target=\"_blank\">jolly-violet-30</a></strong> to <a href='https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2' target=\"_blank\">https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2/runs/7ofgzkbv' target=\"_blank\">https://wandb.ai/da24m029-indian-institute-of-technology-madras/da24m029-da6401-assignment2/runs/7ofgzkbv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal preprocessing, no augmentation\n",
      "Total images: 9999\n",
      "Training set: 7999 images\n",
      "Validation set: 2000 images\n",
      "Number of classes: 10\n",
      "Model: resnet50, Strategy: last_only → Ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 125/125 [01:01<00:00,  2.03it/s]\n",
      "Eval Epoch 1: 100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "Train Epoch 2: 100%|██████████| 125/125 [01:01<00:00,  2.03it/s]\n",
      "Eval Epoch 2: 100%|██████████| 32/32 [00:24<00:00,  1.32it/s]\n",
      "Train Epoch 3: 100%|██████████| 125/125 [01:02<00:00,  2.00it/s]\n",
      "Eval Epoch 3: 100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "Train Epoch 4: 100%|██████████| 125/125 [01:03<00:00,  1.97it/s]\n",
      "Eval Epoch 4: 100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "Train Epoch 5: 100%|██████████| 125/125 [01:02<00:00,  1.99it/s]\n",
      "Eval Epoch 5: 100%|██████████| 32/32 [00:25<00:00,  1.26it/s]\n",
      "Train Epoch 6: 100%|██████████| 125/125 [01:03<00:00,  1.97it/s]\n",
      "Eval Epoch 6: 100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "Train Epoch 7: 100%|██████████| 125/125 [01:02<00:00,  1.99it/s]\n",
      "Eval Epoch 7: 100%|██████████| 32/32 [00:24<00:00,  1.31it/s]\n",
      "Train Epoch 8: 100%|██████████| 125/125 [02:24<00:00,  1.16s/it]\n",
      "Eval Epoch 8: 100%|██████████| 32/32 [00:28<00:00,  1.10it/s]\n",
      "Train Epoch 9: 100%|██████████| 125/125 [02:19<00:00,  1.12s/it]\n",
      "Eval Epoch 9: 100%|██████████| 32/32 [00:34<00:00,  1.09s/it]\n",
      "Train Epoch 10: 100%|██████████| 125/125 [02:21<00:00,  1.14s/it]\n",
      "Eval Epoch 10: 100%|██████████| 32/32 [00:34<00:00,  1.06s/it]\n",
      "Train Epoch 11: 100%|██████████| 125/125 [02:16<00:00,  1.09s/it]\n",
      "Eval Epoch 11: 100%|██████████| 32/32 [00:34<00:00,  1.06s/it]\n",
      "Train Epoch 12: 100%|██████████| 125/125 [02:06<00:00,  1.01s/it]\n",
      "Eval Epoch 12: 100%|██████████| 32/32 [00:33<00:00,  1.06s/it]\n",
      "Train Epoch 13: 100%|██████████| 125/125 [02:05<00:00,  1.00s/it]\n",
      "Eval Epoch 13: 100%|██████████| 32/32 [00:39<00:00,  1.24s/it]\n",
      "Train Epoch 14: 100%|██████████| 125/125 [02:06<00:00,  1.02s/it]\n",
      "Eval Epoch 14: 100%|██████████| 32/32 [00:36<00:00,  1.13s/it]\n",
      "Train Epoch 15: 100%|██████████| 125/125 [02:05<00:00,  1.00s/it]\n",
      "Eval Epoch 15: 100%|██████████| 32/32 [00:37<00:00,  1.17s/it]\n",
      "Train Epoch 16: 100%|██████████| 125/125 [02:03<00:00,  1.02it/s]\n",
      "Eval Epoch 16: 100%|██████████| 32/32 [00:34<00:00,  1.06s/it]\n",
      "Train Epoch 17: 100%|██████████| 125/125 [02:15<00:00,  1.08s/it]\n",
      "Eval Epoch 17: 100%|██████████| 32/32 [00:33<00:00,  1.05s/it]\n",
      "Train Epoch 18: 100%|██████████| 125/125 [02:21<00:00,  1.13s/it]\n",
      "Eval Epoch 18: 100%|██████████| 32/32 [00:34<00:00,  1.07s/it]\n",
      "Train Epoch 19: 100%|██████████| 125/125 [02:19<00:00,  1.12s/it]\n",
      "Eval Epoch 19: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]\n",
      "Train Epoch 20: 100%|██████████| 125/125 [02:18<00:00,  1.11s/it]\n",
      "Eval Epoch 20: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s]\n",
      "Train Epoch 21: 100%|██████████| 125/125 [02:26<00:00,  1.18s/it]\n",
      "Eval Epoch 21: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
      "Train Epoch 22: 100%|██████████| 125/125 [02:19<00:00,  1.12s/it]\n",
      "Eval Epoch 22: 100%|██████████| 32/32 [00:27<00:00,  1.18it/s]\n",
      "Train Epoch 23: 100%|██████████| 125/125 [02:21<00:00,  1.13s/it]\n",
      "Eval Epoch 23: 100%|██████████| 32/32 [00:28<00:00,  1.13it/s]\n",
      "Train Epoch 24: 100%|██████████| 125/125 [03:31<00:00,  1.69s/it]\n",
      "Eval Epoch 24: 100%|██████████| 32/32 [00:28<00:00,  1.13it/s]\n",
      "Train Epoch 25: 100%|██████████| 125/125 [01:06<00:00,  1.88it/s]\n",
      "Eval Epoch 25: 100%|██████████| 32/32 [00:26<00:00,  1.23it/s]\n",
      "Train Epoch 26: 100%|██████████| 125/125 [01:04<00:00,  1.94it/s]\n",
      "Eval Epoch 26: 100%|██████████| 32/32 [00:24<00:00,  1.30it/s]\n",
      "Train Epoch 27: 100%|██████████| 125/125 [01:13<00:00,  1.71it/s]\n",
      "Eval Epoch 27: 100%|██████████| 32/32 [00:25<00:00,  1.25it/s]\n",
      "Train Epoch 28: 100%|██████████| 125/125 [01:04<00:00,  1.93it/s]\n",
      "Eval Epoch 28: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s]\n",
      "Train Epoch 29: 100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n",
      "Eval Epoch 29: 100%|██████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "Train Epoch 30: 100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n",
      "Eval Epoch 30: 100%|██████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "Train Epoch 31: 100%|██████████| 125/125 [01:11<00:00,  1.75it/s]\n",
      "Eval Epoch 31: 100%|██████████| 32/32 [00:26<00:00,  1.22it/s]\n",
      "Train Epoch 32: 100%|██████████| 125/125 [01:04<00:00,  1.94it/s]\n",
      "Eval Epoch 32: 100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n",
      "Train Epoch 33: 100%|██████████| 125/125 [01:06<00:00,  1.88it/s]\n",
      "Eval Epoch 33: 100%|██████████| 32/32 [00:27<00:00,  1.16it/s]\n",
      "Train Epoch 34: 100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "Eval Epoch 34: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s]\n",
      "Train Epoch 35: 100%|██████████| 125/125 [01:10<00:00,  1.78it/s]\n",
      "Eval Epoch 35: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s]\n",
      "Train Epoch 36: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s]\n",
      "Eval Epoch 36: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]\n",
      "Train Epoch 37: 100%|██████████| 125/125 [01:15<00:00,  1.65it/s]\n",
      "Eval Epoch 37: 100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n",
      "Train Epoch 38: 100%|██████████| 125/125 [01:11<00:00,  1.74it/s]\n",
      "Eval Epoch 38: 100%|██████████| 32/32 [00:27<00:00,  1.15it/s]\n",
      "Train Epoch 39: 100%|██████████| 125/125 [01:08<00:00,  1.81it/s]\n",
      "Eval Epoch 39: 100%|██████████| 32/32 [00:26<00:00,  1.23it/s]\n",
      "Train Epoch 40: 100%|██████████| 125/125 [01:07<00:00,  1.84it/s]\n",
      "Eval Epoch 40: 100%|██████████| 32/32 [00:26<00:00,  1.22it/s]\n",
      "Train Epoch 41: 100%|██████████| 125/125 [01:05<00:00,  1.92it/s]\n",
      "Eval Epoch 41: 100%|██████████| 32/32 [00:26<00:00,  1.21it/s]\n",
      "Train Epoch 42: 100%|██████████| 125/125 [01:10<00:00,  1.76it/s]\n",
      "Eval Epoch 42:  47%|████▋     | 15/32 [00:21<00:24,  1.45s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\storage.py\", line 1198, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\storage.py\", line 413, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <torch_35196_3377509879_14>, error code: <1455>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_finetune_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpercent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m, in \u001b[0;36mrun_finetune_training\u001b[1;34m(backbone, strategy, num_epochs, batch_size, k, percent, num_classes)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     38\u001b[0m     train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n\u001b[1;32m---> 39\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(best_model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartB.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     42\u001b[0m wandb\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_B.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, criterion, device, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m val_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     26\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32md:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32md:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\storage.py\", line 1198, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n  File \"d:\\Jamie\\miniconda3\\envs\\env2\\lib\\site-packages\\torch\\storage.py\", line 413, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <torch_35196_3377509879_14>, error code: <1455>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "finetuned_model = run_finetune_training(\n",
    "                    backbone=\"resnet50\",\n",
    "                    strategy=\"last_only\",\n",
    "                    percent=0.6,\n",
    "                    num_epochs=100,\n",
    "                    num_classes=10,\n",
    "                    batch_size=64,\n",
    "                    k=10,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c535c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # match training image size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # match training normalization\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir,'val'), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147c13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Test Accuracy: 0.6090\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = finetuned_model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Print it\n",
    "print(f\" Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Log to wandb (if active)\n",
    "if wandb.run is not None:\n",
    "    wandb.log({\"test_accuracy\": test_accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c926b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
